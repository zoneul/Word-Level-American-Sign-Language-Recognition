{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q mediapipe==0.10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import mediapipe as mp\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hand = list(range(21))\n",
    "\n",
    "filtered_pose = [11, 12, 13, 14, 15, 16]\n",
    "\n",
    "filtered_face = [0, 4, 7, 8, 10, 13, 14, 17, 21, 33, 37, 39, 40, 46, 52, 53, 54, 55, 58,\n",
    "                 61, 63, 65, 66, 67, 70, 78, 80, 81, 82, 84, 87, 88, 91, 93, 95, 103, 105,\n",
    "                 107, 109, 127, 132, 133, 136, 144, 145, 146, 148, 149, 150, 152, 153, 154,\n",
    "                 155, 157, 158, 159, 160, 161, 162, 163, 172, 173, 176, 178, 181, 185, 191,\n",
    "                 234, 246, 249, 251, 263, 267, 269, 270, 276, 282, 283, 284, 285, 288, 291,\n",
    "                 293, 295, 296, 297, 300, 308, 310, 311, 312, 314, 317, 318, 321, 323, 324,\n",
    "                 332, 334, 336, 338, 356, 361, 362, 365, 373, 374, 375, 377, 378, 379, 380,\n",
    "                 381, 382, 384, 385, 386, 387, 388, 389, 390, 397, 398, 400, 402, 405, 409,\n",
    "                 415, 454, 466, 468, 473]\n",
    "\n",
    "HAND_NUM = len(filtered_hand)\n",
    "POSE_NUM = len(filtered_pose)\n",
    "FACE_NUM = len(filtered_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hands = mp.solutions.hands.Hands()\n",
    "pose = mp.solutions.pose.Pose()\n",
    "face_mesh = mp.solutions.face_mesh.FaceMesh(refine_landmarks=True)\n",
    "\n",
    "def get_frame_landmarks(frame):\n",
    "    \n",
    "    all_landmarks = np.zeros((HAND_NUM * 2 + POSE_NUM + FACE_NUM, 3))\n",
    "    \n",
    "    def get_hands(frame):\n",
    "        results_hands = hands.process(frame)\n",
    "        if results_hands.multi_hand_landmarks:\n",
    "            for i, hand_landmarks in enumerate(results_hands.multi_hand_landmarks):\n",
    "                if results_hands.multi_handedness[i].classification[0].index == 0: \n",
    "                    all_landmarks[:HAND_NUM, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # right\n",
    "                else:\n",
    "                    all_landmarks[HAND_NUM:HAND_NUM * 2, :] = np.array(\n",
    "                        [(lm.x, lm.y, lm.z) for lm in hand_landmarks.landmark]) # left\n",
    "\n",
    "    def get_pose(frame):\n",
    "        results_pose = pose.process(frame)\n",
    "        if results_pose.pose_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2:HAND_NUM * 2 + POSE_NUM, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_pose.pose_landmarks.landmark])[filtered_pose]\n",
    "        \n",
    "    def get_face(frame):\n",
    "        results_face = face_mesh.process(frame)\n",
    "        if results_face.multi_face_landmarks:\n",
    "            all_landmarks[HAND_NUM * 2 + POSE_NUM:, :] = np.array(\n",
    "                [(lm.x, lm.y, lm.z) for lm in results_face.multi_face_landmarks[0].landmark])[filtered_face]\n",
    "        \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        executor.submit(get_hands, frame)\n",
    "        executor.submit(get_pose, frame)\n",
    "        executor.submit(get_face, frame)\n",
    "\n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_landmarks(video_path, start_frame=1, end_frame=-1):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # if the starting is 0\n",
    "    if start_frame <= 1:\n",
    "        start_frame = 1\n",
    "        \n",
    "    # if the video is precropped\n",
    "    elif start_frame > int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        start_frame = 1\n",
    "        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "    # if the final frame was not given (-1)    \n",
    "    if end_frame < 0: \n",
    "        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    num_landmarks = HAND_NUM * 2 + POSE_NUM + FACE_NUM\n",
    "    all_frame_landmarks = np.zeros((end_frame - start_frame + 1, num_landmarks, 3))\n",
    "    frame_index = 1\n",
    "    \n",
    "    while cap.isOpened() and frame_index <= end_frame:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        if frame_index >= start_frame:\n",
    "            frame.flags.writeable = False\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame_landmarks = get_frame_landmarks(frame)\n",
    "            all_frame_landmarks[frame_index - start_frame] = frame_landmarks\n",
    "\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    hands.reset()\n",
    "    pose.reset()\n",
    "    face_mesh.reset()\n",
    "    return all_frame_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(input_path, output_path, video_landmarks, start_frame=1, end_frame=-1):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    if start_frame <= 1:\n",
    "        start_frame = 1\n",
    "    elif start_frame > int(cap.get(cv2.CAP_PROP_FRAME_COUNT)):\n",
    "        start_frame = 1\n",
    "        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if end_frame < 0:\n",
    "        end_frame = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "    frame_index = 1\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_index >= start_frame and frame_index <= end_frame:\n",
    "            frame_landmarks = video_landmarks[frame_index - start_frame]\n",
    "            landmarks = [(int(x * width), int(y * height)) for x, y, _ in frame_landmarks]\n",
    "            for x, y in landmarks:\n",
    "                cv2.circle(frame, (x, y), 3, (0, 0, 255), -1)\n",
    "            out.write(frame)\n",
    "        else:\n",
    "            # out.write(frame) # Enable if you want the full video\n",
    "            pass\n",
    "        frame_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "npy_dir = 'landmarks'\n",
    "os.makedirs(npy_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('WLASL_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 28642.77it/s]\n"
     ]
    }
   ],
   "source": [
    "co = {}\n",
    "try:\n",
    "    for i in tqdm(range(len(data)), ncols=100):\n",
    "        gloss = data[i]['gloss']\n",
    "        if gloss not in co:\n",
    "            co[gloss] = 1\n",
    "        else:\n",
    "            co[gloss] += 1\n",
    "        \n",
    "        npy_path = os.path.join(npy_dir, f\"{gloss}{co[gloss]}.npy\")\n",
    "        if os.path.exists(npy_path):\n",
    "            continue\n",
    "        \n",
    "        video_path = data[i]['video_path']\n",
    "        start = data[i]['frame_start']\n",
    "        end = data[i]['frame_end']\n",
    "        \n",
    "        try:\n",
    "            video_landmarks = get_video_landmarks(video_path, start, end)\n",
    "            np.save(npy_path, video_landmarks)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError encoding {video_path}\\n{e}\")\n",
    "            continue\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nLoading process interrupted by user.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_dict = {}\n",
    "\n",
    "for filename in os.listdir(npy_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        key = filename.split('.')[0]\n",
    "        landmarks = np.load(os.path.join(npy_dir, filename), allow_pickle=True)\n",
    "        landmarks_dict[key] = landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the word list\n",
    "with open('wordlist.txt', 'r') as f:\n",
    "    word_index = {word:index for index, word in enumerate([line.strip() for line in f])}\n",
    "    reverse_word_index = {index: word for word, index in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "X, y = [], []\n",
    "for i in landmarks_dict:\n",
    "    for j in landmarks_dict[i]:\n",
    "        X.append(j)\n",
    "        y.append(np.eye(len(word_index), dtype=int, k=word_index[re.sub(r'\\d+$', '', i)])[0])\n",
    "X, y = np.array(X), np.array(y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (80%, 20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 180, 512)          2048      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 180, 512)          0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 180, 256)          131328    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 180, 256)          0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 180, 128)          32896     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 180, 128)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 23040)             0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                368656    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 534,928\n",
      "Trainable params: 534,928\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models  import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=X.shape[1:], activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(len(word_index), activation='softmax'))\n",
    "\n",
    "initial_learning_rate = 0.001\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate, decay_steps=10000, decay_rate=0.9)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='loss',  # Metric to monitor for early stopping\n",
    "    mode='min',  # Set mode to 'min' for minimizing the metric\n",
    "    patience=15,  # Number of epochs with no improvement before stopping\n",
    "    restore_best_weights=True,  # Restore the best model weights\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=lr_schedule), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "101/101 [==============================] - 28s 275ms/step - loss: 2.2096 - accuracy: 0.2822\n",
      "Epoch 2/30\n",
      "101/101 [==============================] - 27s 270ms/step - loss: 1.6581 - accuracy: 0.4667\n",
      "Epoch 3/30\n",
      "101/101 [==============================] - 27s 270ms/step - loss: 1.4265 - accuracy: 0.5473\n",
      "Epoch 4/30\n",
      "101/101 [==============================] - 28s 272ms/step - loss: 1.2892 - accuracy: 0.5839\n",
      "Epoch 5/30\n",
      "101/101 [==============================] - 27s 271ms/step - loss: 1.2013 - accuracy: 0.6148\n",
      "Epoch 6/30\n",
      "101/101 [==============================] - 28s 279ms/step - loss: 1.1268 - accuracy: 0.6352\n",
      "Epoch 7/30\n",
      "101/101 [==============================] - 27s 271ms/step - loss: 1.0719 - accuracy: 0.6495\n",
      "Epoch 8/30\n",
      "101/101 [==============================] - 27s 272ms/step - loss: 1.0409 - accuracy: 0.6586\n",
      "Epoch 9/30\n",
      "101/101 [==============================] - 27s 272ms/step - loss: 0.9979 - accuracy: 0.6673\n",
      "Epoch 10/30\n",
      "101/101 [==============================] - 27s 270ms/step - loss: 0.9448 - accuracy: 0.6826\n",
      "Epoch 11/30\n",
      "101/101 [==============================] - 27s 270ms/step - loss: 0.9190 - accuracy: 0.6950\n",
      "Epoch 12/30\n",
      "101/101 [==============================] - 27s 272ms/step - loss: 0.8824 - accuracy: 0.7061\n",
      "Epoch 13/30\n",
      "101/101 [==============================] - 28s 272ms/step - loss: 0.8686 - accuracy: 0.7107\n",
      "Epoch 14/30\n",
      "101/101 [==============================] - 27s 270ms/step - loss: 0.8349 - accuracy: 0.7191\n",
      "Epoch 15/30\n",
      "101/101 [==============================] - 28s 273ms/step - loss: 0.8113 - accuracy: 0.7293\n",
      "Epoch 16/30\n",
      "101/101 [==============================] - 28s 279ms/step - loss: 0.7988 - accuracy: 0.7299\n",
      "Epoch 17/30\n",
      "101/101 [==============================] - 28s 277ms/step - loss: 0.7559 - accuracy: 0.7425\n",
      "Epoch 18/30\n",
      "101/101 [==============================] - 28s 282ms/step - loss: 0.7380 - accuracy: 0.7522\n",
      "Epoch 19/30\n",
      "101/101 [==============================] - 28s 281ms/step - loss: 0.7195 - accuracy: 0.7582\n",
      "Epoch 20/30\n",
      "101/101 [==============================] - 28s 278ms/step - loss: 0.7023 - accuracy: 0.7600\n",
      "Epoch 21/30\n",
      "101/101 [==============================] - 28s 280ms/step - loss: 0.6814 - accuracy: 0.7730\n",
      "Epoch 22/30\n",
      "101/101 [==============================] - 28s 282ms/step - loss: 0.6480 - accuracy: 0.7795\n",
      "Epoch 23/30\n",
      "101/101 [==============================] - 28s 282ms/step - loss: 0.6415 - accuracy: 0.7842\n",
      "Epoch 24/30\n",
      "101/101 [==============================] - 29s 284ms/step - loss: 0.6191 - accuracy: 0.7917\n",
      "Epoch 25/30\n",
      "101/101 [==============================] - 29s 284ms/step - loss: 0.6086 - accuracy: 0.7935\n",
      "Epoch 26/30\n",
      "101/101 [==============================] - 28s 282ms/step - loss: 0.6110 - accuracy: 0.7947\n",
      "Epoch 27/30\n",
      "101/101 [==============================] - 28s 279ms/step - loss: 0.5679 - accuracy: 0.8062\n",
      "Epoch 28/30\n",
      "101/101 [==============================] - 28s 278ms/step - loss: 0.5703 - accuracy: 0.8079\n",
      "Epoch 29/30\n",
      "101/101 [==============================] - 28s 275ms/step - loss: 0.5598 - accuracy: 0.8092\n",
      "Epoch 30/30\n",
      "101/101 [==============================] - 28s 276ms/step - loss: 0.5438 - accuracy: 0.8132\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "model_training_history = model.fit(X_train, y_train, batch_size=128, epochs=30 , callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.377358, Accuracy: 0.876121\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on train data\n",
    "print('Train Loss: {:.6f}, Accuracy: {:.6f}'.format(*model.evaluate(X_train, y_train, verbose=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.434277, Accuracy: 0.859282\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "print('Test Loss: {:.6f}, Accuracy: {:.6f}'.format(*model.evaluate(X_test, y_test, verbose=0)))\n",
    "model_evaluation_history = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-time section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Start capturing video from the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Get the height and width of the frame\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Get landmarks for the frame\n",
    "    frame_landmarks = get_frame_landmarks(frame)\n",
    "\n",
    "    # Draw landmarks on the frame\n",
    "    for landmark in frame_landmarks:\n",
    "        x = int(landmark[0] * width)\n",
    "        y = int(landmark[1] * height)\n",
    "        cv2.circle(frame, (x, y), 2, (0, 255, 0), -1)\n",
    "\n",
    "    frame_pred = model.predict(np.array([frame_landmarks]), verbose=0)\n",
    "    frame_word_index = np.argmax(frame_pred)\n",
    "    probability = round(np.max(frame_pred)*100,2)\n",
    "    \n",
    "    # Add text to the frame\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(frame, reverse_word_index[frame_word_index] + ': ' + str(probability) + '%', (10, 30), font, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Frame with Landmarks', frame)\n",
    "\n",
    "    # Check for the 'q' key to quit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
